{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "OLD_PATH = \"/data/gentrification/buffalo_oarsystem_data\"\n",
    "DATA_DIR = \"/data/gentrification/LATEST\"\n",
    "\n",
    "# open data data frame \n",
    "open_data_df = pd.read_csv('https://data.buffalony.gov/api/views/kckn-jafw/rows.csv?accessType=DOWNLOAD',low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we import all function used \n",
    "%run util_datacollection.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Find all SBLs (unique parcel identifiers) in Erie county data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "ALL_SBL_FILE = os.path.join(DATA_DIR,\"owner_history_htmls\")\n",
    "lists_of_sbls_htmls = os.listdir(ALL_SBL_FILE)\n",
    "possibilities =  [char for char in string.digits + string.ascii_uppercase] # possible combinations \n",
    "\n",
    "if os.path.exists(ALL_SBL_FILE):\n",
    "    all_sbls = [x.strip().split('.')[0].replace('#','.').replace('+','/') for x in lists_of_sbls_htmls]\n",
    "    print(\"ALREADY RAN THIS STEP!\")\n",
    "\n",
    "else:\n",
    "    all_sbls = []\n",
    "    for letter in possibilities:\n",
    "        all_sbls += grab_source(letter,path = DATA_DIR) # creates a new directory and appends htmls files onto the created directories \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download and compile all owner pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_name =  'owner_history.csv'\n",
    "csv_owner_history_file_path  = os.path.join(DATA_DIR,csv_file_name)\n",
    "if not os.path.exists(csv_owner_history_file_path): # if it has not downloaded it yet \n",
    "    list_of_failed_sbls = html_csv_converter(ALL_SBL_FILE,csv_file_name) # this is to find out SBLS that gave a server error 500\n",
    "else:\n",
    "    print('Owner page history has been made')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owner_history_df = pd.read_csv(csv_owner_history_file_path,names= ['Owner','Owner Name','Book-Page/Date' \n",
    "                                                  ,'Book-Page/Date identification','SBL'])\n",
    "owner_history_error_reformat(owner_history_df,open_data_df,inplace = True) # we then remove any noise and corrupted data (i.g. NAN) and replace them with the information open data buffalo has provided "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scrape the considerations page using the Book-Page column from the owner history \n",
    "\n",
    "## link can be found on http://ecclerk.erie.gov/or_wb1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_CONSIDERATIONS_FILE = os.path.join(DATA_DIR,\"considerations_htmls\")\n",
    "if not os.path.exists(ALL_CONSIDERATIONS_FILE): \n",
    "    scrape_considerations_page(owner_history_df,DATA_DIR) # creates a \n",
    "                                            # new directory called considerations_htmls and puts the newly considerations\n",
    "else:\n",
    "    print(\"CONSIDERATIONS HAVE BEEN SCRAPED\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Concat these into a single CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'complete_search_parcel.csv'\n",
    "path_joined_concatenated = os.path.join(DATA_DIR,'complete_search_parcel.csv')\n",
    "if not os.path.exists(path_joined_concatenated):\n",
    "\n",
    "    # we create a new csv called \"complete_search_parcel.csv\"\n",
    "    list_of_failed_deeds = combine_considerations_owner_history(DATA_DIR,file_name)\n",
    "print(\"CONSIDERATIONS AND OWNER HISTORY CSVS HAVE BEEN CONCATENATED\")\n",
    "complete_dataframe = pd.read_csv(path_joined_concatenated)\n",
    "complete_dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Link to location data w/ Open Data Buffalo \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_link = link_locations(complete_dataframe,open_data_df)\n",
    "locations_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fix location strings using OARSYSTEM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of Locations we are missing prior: \" + str(locations_link['Location'].isna().sum()))\n",
    "oarsystem_data_path = os.path.join(OLD_PATH,'buffalo_oarsystem_CSVs/combined_tax_data.csv')\n",
    "oarsystem_df = pd.read_csv(os.path.join(oarsystem_data_path)).rename(columns = {'SBL':'sbl'})\n",
    "oarsystem_df # note this was scraped before hand; the aim is to take ADDRESS here \n",
    "\n",
    "# I took the address from OARSYSTEM, because the data from the real property tax parcel does not have information such as the ZipCode of a SBL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_dataframe =  batch_geoencode_missing_location(locations_link,OLD_PATH)\n",
    "print('-------------------------------------------------------------------------')\n",
    "print(\"Number of missing locations now.. : \" + str(fixed_dataframe['Location'].isna().sum()))\n",
    "fixed_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataframe = add_multiple_doc_types_column(fix_dates(fixed_dataframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Filter out homes from apartments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.exists('filtered_data.csv'):\n",
    "    print('filtered data already exists')\n",
    "    filtered_datav = pd.read_csv('filtered_data.csv')\n",
    "else:\n",
    "    filtered_datav, failed_files, anomolies = filter_apartments(new_dataframe,DATA_DIR)\n",
    "    filtered_datav.to_csv('filtered_data.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
